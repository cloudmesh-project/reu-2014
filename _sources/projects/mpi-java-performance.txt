Evaluating the Performance of MPI Java in FutureGrid and Big Red2
======================================================================

Abstract
---------------------------------------------------------------------
Evaluating the Performance of MPi Java in FutureGrid and Big Red2

Mentor:  Saliya Ekanayake

Team Members: Tori Wilbon, Nigel Pugh


Message Passing Interface (MPI) has become the de facto way of implementing High Performance Computing (HPC) applications as it provides library routines to satisfy virtually all communication patterns. The goal of our research is to understand performance characteristics of parallel programs written in Java versions of MPI. Two such MPI libraries exist, i.e. OpenMPI and FastMPJ. The team will be testing performance of our applications based on both these frameworks. The algorithms that the team will be measuring are a complex and O(N^2) complexities. The team needs to make sure they run in an optimal fashion and give us the best results. The other aspect to our project is to develop some sample MPI Java programs.

Keywords: MPI, Java, Linux, Pairwise Cluster, FutureGrid, Big Red2







Team
----------------------------------------------------------------------



Nigel Pugh
nigel.pugh32@gmail.com


Problem
----------------------------------------------------------------------

a) Testing Performance of MPI Program





Design 
----------------------------------------------------------------------

The purpose of my project is to measure the preformance of MPI Java and record the results. We want to make sure that the programs are running in optimal fashion and giving the best results possible

Implementation
----------------------------------------------------------------------


Links
----------------------------------------------------------------------


Progress Report:


Progress:
---------------------------------------------------------------------- 
[done, Week 1] Learned java programming language

[done, Week 1] Learned about Linux and command line terminal

[done, Week 1] Obtained an understanding of what my project is about.

[done, Week 2] Install MPI program on local machine.

[done, Week 2] Ran a sample MPI program

[done, Week 2] Updated abstracts and bios on github.

[done, Week 2] Created ssh keys for FutureGrid.

Problems:
----------------------------------------------------------------------
All goals and plans were met.

Plan:

[done, Week 1] Setup MPI framework in local machine

[done, Week 1] Run a sample MPI program local machine

[done, Week 1] Do 1 and 2 in a cluster instead of local machine

[       Week 2] Install MPI on FutureGrid.

[       Week 2] Run program on MPI on FutureGrid
